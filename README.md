# ğŸš› Air Pressure System (APS) Failure Prediction â€“ End-to-End MLOps Production Grade Project

## ğŸ§  Overview

This project is a **production-grade MLOps pipeline** for predicting failures in the **Air Pressure System (APS)** of heavy-duty trucks.  
The APS supplies pressurized air used in braking, gear shifting, and other critical functions. The objective is to minimize operational costs by **reducing false predictions** â€” especially **false negatives**, which represent undetected APS failures and can cause expensive breakdowns.

---
> âš™ï¸ This project is built with a complete **MLOps production pipeline** using **Docker**, **AWS S3**, **AWS EC2**, **AWS ECR**, and **GitHub Actions** for automated **CI/CD deployment**.  
It ensures continuous integration, continuous training, and continuous deployment with containerized builds and cloud-based orchestration.

---
## ğŸ§© Problem Statement

The APS dataset contains **sensor readings** from the truckâ€™s air pressure system.

- **Positive Class (1):** Trucks with APS component failure.  
- **Negative Class (0):** Trucks with failures unrelated to APS.  

### ğŸ¯ Business Objective

Misclassification costs are **asymmetric**:

- **Cost_1 (False Positive)** = 10 (unnecessary mechanic check/repair)  
- **Cost_2 (False Negative)** = 500 (missed APS failure â†’ possible breakdown)

**Total Cost formula:**

\[
\text{Total Cost} = (10 \times \text{False Positives}) + (500 \times \text{False Negatives})
\]

Because `Cost_2` is 50Ã— `Cost_1`, the pipeline is optimized to **minimize false negatives** first and then reduce false positives.

---

## ğŸ—ï¸ Folder Structure

## ğŸ“ Project Structure

```bash
sensor_project/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ main.yaml
â”‚
â”œâ”€â”€ sensor/
â”‚   â”œâ”€â”€ cloud_storage/
â”‚   â”‚   â””â”€â”€ s3_syncer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ configuration/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ mongo_db_connection.py
â”‚   â”‚
â”‚   â”œâ”€â”€ constant/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ application.py
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â”œâ”€â”€ env_variable.py
â”‚   â”‚   â”œâ”€â”€ s3_bucket.py
â”‚   â”‚   â””â”€â”€ training_pipeline/
â”‚   â”‚       â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data_access/
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ entity/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ artifact_entity.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”œâ”€â”€ metric/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ classification_metric.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ estimator.py
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ training_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ main_utils.py
â”‚   â”‚
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---
## âš¡ FastAPI Integration â€“ Real-Time Inference API

To serve predictions via REST, this project integrates **FastAPI** for scalable, low-latency inference.


---
## ğŸ”„ CI/CD Pipeline (GitHub Actions + AWS)

**Workflow:** `.github/workflows/main.yaml`

**Pipeline stages:**
1. **Build & Lint**
   - Install dependencies from `requirements.txt`
   - Run linters and static checks (flake8/black)
2. **Test**
   - Run unit tests and integration tests
3. **Train** (optional / conditional)
   - Execute training pipeline (if data or code changes warrant retraining)
   - Generate artifacts under `artifact/`
4. **Evaluation & Validation**
   - Run evaluation scripts to compute metrics and custom cost
   - Validate model meets thresholds (e.g., total cost < threshold OR FN rate <= limit)
5. **Sync & Deploy**
   - Upload model, transformer, and artifacts to **AWS S3**
   - (Optional) Trigger deployment to EC2 / Lambda / SageMaker


**Note:** Secure secrets using GitHub Secrets (e.g., `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `MONGO_DB_URL`).

---

## â˜ï¸ AWS Integration

- **S3** â€” primary artifact/model storage (trained models, transformers, metrics, logs)  
- **IAM** â€” least-privilege roles for CI/CD and runtime access  
- **EC2** â€” optional model serving options (batch/real-time)  
- **CloudWatch / S3 Events** â€” optional monitoring + retrain triggers

Common pattern:
- CI pipeline builds and tests â†’ artifacts saved to `artifact/` â†’ `s3_syncer.py` uploads artifacts to `s3://<bucket>/models/<run-id>/`

---

## ğŸ§® Schema Configuration

**File:** `config/schema.yaml` â€” controls ingestion + validation

Contents include:
- `columns` â€” list of all columns with types
- `numerical_columns` â€” features used for transformation/scaling
- `drop_columns` â€” columns to remove before training
- (Optional) `target_column` â€” explicit target column (e.g., `class`)

**Usage:**
- `Data Ingestion` reads schema and validates incoming data shape & dtypes.
- `Data Validation` will fail the pipeline if required columns missing or unexpected types present.

---

## ğŸ§  Machine Learning Pipeline (Detailed Steps)

### 1ï¸âƒ£ Data Ingestion
- Read raw sensor data from **MongoDB** (via `mongo_db_connection.py`)
- Export collection to CSV / DataFrame
- Save raw artifact to `artifact/raw/` and optionally to **AWS S3**
- Split into train/test according to configuration (e.g., 80/20)

---

### 2ï¸âƒ£ Data Transformation
- Handle missing values (impute or fill)
- Scale numeric features using **RobustScaler / StandardScaler**
- Encode categorical variables if present
- Save transformer object (`transformer.pkl`) to `artifact/` and **S3**

---

### 3ï¸âƒ£ Model Training
- Train multiple candidate models (e.g., **LogisticRegression**, **RandomForest**, **XGBoost**)
- Train models on transformed training data
- Save the best-performing model wrapped in a **SensorModel** class (includes both model and preprocessor)
- Generate `model.pkl` under `artifact/model/` and optionally sync to **S3**

---

### 4ï¸âƒ£ Evaluation
- Combine train & test data or evaluate on a dedicated holdout test set
- Compute model predictions and classic metrics: **Precision**, **Recall**, **F1-Score**
- **Compute custom cost:**
  \[
  \text{Total Cost} = (10 \times \text{False Positives}) + (500 \times \text{False Negatives})
  \]
- Select the model with **minimum total cost** and acceptable recall threshold

---

### 5ï¸âƒ£ Model Pushing
- Once the model passes all evaluation checks:
  - Push the trained model (`model.pkl`), transformer (`transformer.pkl`), and metrics to **AWS S3** under a versioned directory (e.g., `s3://<bucket>/models/<timestamp>/`)
  - Containerize the inference environment using **Docker**
  - Build and push the Docker image to **AWS ECR (Elastic Container Registry)**
  - Deploy the containerized model to **AWS EC2** or other compute service for real-time inference
- The model version and metadata are tracked for reproducibility and rollback

