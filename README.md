# ğŸš› Air Pressure System (APS) Failure Prediction â€“ End-to-End MLOps Production Grade Project

## ğŸ§  Overview

This project is a **production-grade MLOps pipeline** for predicting failures in the **Air Pressure System (APS)** of heavy-duty trucks.  
The APS supplies pressurized air used in braking, gear shifting, and other critical functions. The objective is to minimize operational costs by **reducing false predictions** â€” especially **false negatives**, which represent undetected APS failures and can cause expensive breakdowns.

---

## ğŸ§© Problem Statement

The APS dataset contains **sensor readings** from the truckâ€™s air pressure system.

- **Positive Class (1):** Trucks with APS component failure.  
- **Negative Class (0):** Trucks with failures unrelated to APS.  

### ğŸ¯ Business Objective

Misclassification costs are **asymmetric**:

- **Cost_1 (False Positive)** = 10 (unnecessary mechanic check/repair)  
- **Cost_2 (False Negative)** = 500 (missed APS failure â†’ possible breakdown)

**Total Cost formula:**

\[
\text{Total Cost} = (10 \times \text{False Positives}) + (500 \times \text{False Negatives})
\]

Because `Cost_2` is 50Ã— `Cost_1`, the pipeline is optimized to **minimize false negatives** first and then reduce false positives.

---

## ğŸ—ï¸ Folder Structure

## ğŸ“ Project Structure

```bash
sensor_project/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ main.yaml
â”‚
â”œâ”€â”€ sensor/
â”‚   â”œâ”€â”€ cloud_storage/
â”‚   â”‚   â””â”€â”€ s3_syncer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ configuration/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ mongo_db_connection.py
â”‚   â”‚
â”‚   â”œâ”€â”€ constant/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ application.py
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â”œâ”€â”€ env_variable.py
â”‚   â”‚   â”œâ”€â”€ s3_bucket.py
â”‚   â”‚   â””â”€â”€ training_pipeline/
â”‚   â”‚       â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data_access/
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ entity/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ artifact_entity.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”œâ”€â”€ metric/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ classification_metric.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ estimator.py
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ training_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ main_utils.py
â”‚   â”‚
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---
## âš¡ FastAPI Integration â€“ Real-Time Inference API

To serve predictions via REST, this project integrates **FastAPI** for scalable, low-latency inference.


---
## ğŸ”„ CI/CD Pipeline (GitHub Actions + AWS)

**Workflow:** `.github/workflows/main.yaml`

**Pipeline stages:**
1. **Build & Lint**
   - Install dependencies from `requirements.txt`
   - Run linters and static checks (flake8/black)
2. **Test**
   - Run unit tests and integration tests
3. **Train** (optional / conditional)
   - Execute training pipeline (if data or code changes warrant retraining)
   - Generate artifacts under `artifact/`
4. **Evaluation & Validation**
   - Run evaluation scripts to compute metrics and custom cost
   - Validate model meets thresholds (e.g., total cost < threshold OR FN rate <= limit)
5. **Sync & Deploy**
   - Upload model, transformer, and artifacts to **AWS S3**
   - (Optional) Trigger deployment to EC2 / Lambda / SageMaker


**Note:** Secure secrets using GitHub Secrets (e.g., `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `MONGO_DB_URL`).

---

## â˜ï¸ AWS Integration

- **S3** â€” primary artifact/model storage (trained models, transformers, metrics, logs)  
- **IAM** â€” least-privilege roles for CI/CD and runtime access  
- **EC2** â€” optional model serving options (batch/real-time)  
- **CloudWatch / S3 Events** â€” optional monitoring + retrain triggers

Common pattern:
- CI pipeline builds and tests â†’ artifacts saved to `artifact/` â†’ `s3_syncer.py` uploads artifacts to `s3://<bucket>/models/<run-id>/`

---

## ğŸ§® Schema Configuration

**File:** `config/schema.yaml` â€” controls ingestion + validation

Contents include:
- `columns` â€” list of all columns with types
- `numerical_columns` â€” features used for transformation/scaling
- `drop_columns` â€” columns to remove before training
- (Optional) `target_column` â€” explicit target column (e.g., `class`)

**Usage:**
- `Data Ingestion` reads schema and validates incoming data shape & dtypes.
- `Data Validation` will fail the pipeline if required columns missing or unexpected types present.

---

## ğŸ§  Machine Learning Pipeline (Detailed Steps)

### 1ï¸âƒ£ Data Ingestion
- Read raw sensor data from **MongoDB** (via `mongo_db_connection.py`)
- Export collection to CSV / DataFrame
- Save raw artifact to `artifact/raw/` and optionally to **S3**
- Split into train/test according to `config` (e.g., 80/20)

### 2ï¸âƒ£ Data Transformation
- Handle missing values (impute or fill)
- Scale numeric features (RobustScaler / StandardScaler)
- Encode categorical variables if present
- Save transformer object (`transformer.pkl`) to `artifact/` and S3

### 3ï¸âƒ£ Model Training
- Train candidate models (e.g., LogisticRegression, RandomForest, XGBoost)
- Train on transformed train set
- Store trained model object (wrapped in `SensorModel` to include preprocessors)

### 4ï¸âƒ£ Evaluation
- Combine train & test or evaluate on holdout test set
- Compute predictions and classic metrics (precision, recall, f1)
- **Compute custom cost**:
